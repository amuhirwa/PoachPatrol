# Summary Table

| Model  | Optimizer | Regularization | Early Stopping | Dropout | Learning Rate | Num of Layers       | Accuracy | Precision | Recall | F1-Score | Best Epoch | Training Loss | Validation Loss |
|--------|-----------|----------------|----------------|---------|---------------|-------------------|----------|-----------|--------|----------|------------|---------------|------------------|
| **model_1**| Adam      | None           | False          | 0.0     | 0.001         | 10     | **0.8036** | 0.8033    | 0.8036 | 0.8032   | 30         | 0.0058       | 1.2060          |
| **model_2**| RMSprop   | L2 (0.01)      | True (pat=5)   | 0.4     | 0.0005        | 10     | 0.6667   | 0.6847    | 0.6667 | 0.6608   | 18         | 0.9430       | 0.9154          |
| **model_3**| Adam      | L2 (0.01)      | True (pat=10)  | 0.2     | 0.001         | 11     | 0.7024   | 0.7107    | 0.7024 | 0.6999   | 20         | 0.8295       | 0.8558          |
| **model_4**| SGD       | L1_L2          | True (pat=5)   | 0.5     | **0.01**      | 11     | **0.7976** | 0.8076    | 0.7976 | 0.7999   | 30         | 1.4965       | 1.8144          |
| **model_5** (RF) | n/a   | n/a            | n/a            | n/a     | n/a           | n/a               | 0.6905   | 0.7204    | 0.6905 | 0.6873   | n/a        | n/a          | n/a              |

## Summary of results
1. **Model 1**: Model one had everything set to defaults (learning rate of 0.001 for Adam) with no optimization or techniques to improve the model used. It achieved an accuracy of 0.8036 which is the highest but it is highly deceiving because of how overfitted it is. Due to having no early stopping, dropout, or regularization, it went past it's minima and kept memorizing data which is evident by the huge gap between the final losses with training loss being at 0.0058 and validation loss being at 1.2060.
As for the error metrics the precision and recall are again high indicating it only shows about 80% of false positives and false negatives. But due to how overfitted it is I wouldn't trust it with outside data as it would probably not be able to generalize.
2. **Model 2**: Model 2, I used some techniques to try to improve the model which greatly helped with the overfitting from the first model. I also used a different  I used the l2 regularizer to penalize any large weights that may arise and thus prevent overfitting, I also used early stopping with a patience of 5 to stop when the model stops progressing and not just keep memorizing it if it's not learning, I used had a relatively low dropout to turn off some neurons through the training but keep the training efficiency by it not being too high.
This resulted in a model that did not overfit shown by the close losses (0.9154, 0.9430) but also didn't perform as well as the others with an accuracy of 0.667. Recall are also close by with a 1/3 of predictions being false negatives and a bit better with the precision showing 32% instead being false positives. I believe this might be due to having a short patience and high dropout, while also having a low learning rate so it was learning slower and might've stopped when still learning.
3. **Model 3**: Model 3 I believed performed the best. I used Adam again as it combines RMSprop learning rate adjustion and momentum (tracks the mean of gradients), which I believe helped. I also had a higher learning rate to allow it proper improvement every epoch, and reduced the dropout because it might have been too harsh dropping almost half of the neurons after every layer. I also brought the early stopping patience up to 10 to give it more time to make sure it has reached the global minima.